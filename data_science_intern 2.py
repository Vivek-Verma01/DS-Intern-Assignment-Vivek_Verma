# -*- coding: utf-8 -*-
"""Data Science Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MLYVXzIFNdLfsIbp3teGBD-pjGZR-af4

Required Libraries
"""

# This project aims to develop a machine learning model to predict equipment energy consumption based on sensor data collected from a smart manufacturing facility. The project includes data preprocessing, feature engineering, exploratory data analysis (EDA), model training (Random Forest and XGBoost), model evaluation, and insights.
# Note- We import essential Python libraries for data manipulation, visualization, model development, and evaluation.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""Convert non-timestamp object columns to numeric and Handle missing values"""

def preprocess_data(df):
    for col in df.columns:
        if df[col].dtype == 'object' and col != 'timestamp':
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # Parse timestamp and extract features
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['month'] = df['timestamp'].dt.month

    # Handle missing values
    df.fillna(df.median(numeric_only=True), inplace=True)
    df.dropna(subset=['equipment_energy_consumption'], inplace=True)

    return df

"""Function to evaluate model performance"""

def evaluate_model(name, y_true, y_pred):
    print(f"{name} Evaluation:")
    print(f"MAE:  {mean_absolute_error(y_true, y_pred):.2f}")
    print(f"RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.2f}")
    print(f"R^2:  {r2_score(y_true, y_pred):.4f}")
    print("-" * 30)

"""Function to plot feature importances"""

def evaluate_model(name, y_true, y_pred):
    print(f"{name} Evaluation:")
    print(f"MAE:  {mean_absolute_error(y_true, y_pred):.2f}")
    print(f"RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.2f}")
    print(f"R^2:  {r2_score(y_true, y_pred):.4f}")
    print("-" * 30)

# Function to plot feature importances
def plot_feature_importance(model, features):
    plt.figure(figsize=(10, 6))
    importances = model.feature_importances_
    sorted_idx = np.argsort(importances)
    sns.barplot(x=importances[sorted_idx], y=features[sorted_idx])
    plt.title("Feature Importances")
    plt.tight_layout()
    plt.show()

"""Load and explore data"""

df = pd.read_csv('/data.csv')
print(df.info())
print(df.describe())
print(df.isnull().sum())

""" Preprocess data"""

df = preprocess_data(df)

""" Exploratory Data Analysis (EDA)"""

# Note- We examine the data using descriptive statistics and visualizations to understand variable relationships and identify patterns.

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), cmap='coolwarm', annot=False)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

"""Investigate correlation with target"""

corr_random_vars = df[['random_variable1', 'random_variable2', 'equipment_energy_consumption']].corr()
print("Correlation with target:\n", corr_random_vars)

"""Prepare features and target"""

drop_cols = ['timestamp', 'equipment_energy_consumption', 'random_variable1', 'random_variable2']
X = df.drop(columns=drop_cols)
y = df['equipment_energy_consumption']

"""Train-Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Random Forest Model"""

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

""" XGBoost Model with Grid Search"""

# Note- We train two models: a baseline Random Forest Regressor and a tuned XGBoost Regressor using GridSearchCV. The models are evaluated using MAE, RMSE, and RÂ².
# Define grid for hyperparameter tuning

grid_params = {
    'n_estimators': [100, 200],         # Number of trees
    'learning_rate': [0.05, 0.1],       # Step size shrinkage
    'max_depth': [3, 5]                 # Tree depth
}


grid_search = GridSearchCV(estimator=XGBRegressor(random_state=42),
                           param_grid=grid_params,
                           cv=5,
                           scoring='neg_root_mean_squared_error',
                           verbose=0)
grid_search.fit(X_train, y_train)

best_xgb = grid_search.best_estimator_
xgb_preds = best_xgb.predict(X_test)

"""Cross-Validation Scores"""

cv_scores = cross_val_score(best_xgb, X_train, y_train, cv=5, scoring='r2')
print(f"XGBoost CV R^2 Scores: {cv_scores}")
print(f"Mean CV R^2 Score: {cv_scores.mean():.4f}")

"""Evaluate Models"""

evaluate_model("Random Forest", y_test, rf_preds)
evaluate_model("XGBoost (Tuned)", y_test, xgb_preds)

"""Feature Importance Plot and save model

"""

plot_feature_importance(best_xgb, X.columns)


# Ensure the directory exists
os.makedirs('outputs/models', exist_ok=True)

# Save the Best Model
joblib.dump(best_xgb, 'outputs/models/xgb_model.pkl')